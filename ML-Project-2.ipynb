{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying COVID19 Papers based on Severity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Luke Batchelder, Jessica Diehl, Drew Griffith and Joe Netti\n",
    "#### CSCI 635 - Introduction to Machine Learning\n",
    "#### Project 2 - COVID-19 Open Research Dataset Challenge\n",
    "#### See on [github](https://github.com/nettijoe96/neural-virus)\n",
    "\n",
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any technical field, researchers must keep pace with new papers. The coronavirus pandemic has generated far more papers than researchers can reasonably sift through. This project aims to help solve this problem that researchers face with the abundance of information using neural networks. Our code was based on being a bricks on bricks operation to other submissions. Based on our sources there seemed to be great tools for sorting the data into groups and generally querying on those groups and there had been solid efforts at ground up NN learning based on ensemble CNN networks. Our code was designed to test the effectiveness of combining these two functionalities by using the very effective code of [3] to find papers that fit our analyses and using the semantic analysis of [1] to find the results in order to classify papers if they discussed patients that discussed severity of hospitalized patients. \n",
    "\n",
    "The task used for this project is the Risk Factors task, specifically, “Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups”. The sentiment analysis is used to provide additional papers other than ones the neural network has been trained to detect.  Does the sentiment within the high severity papers reflect that topics discussed are similar in other papers which have been found to have a similar sentiment?  Especially, do the other papers indicate information about the risk of fatality?  The high risk papers revolve around the severity of disease and contain indications of research involving fatalities or hospitalizations. The neural network used for sentiment analysis classifies papers into discussions about high severity for the disease or no discussion about the severity of the disease.  The aim for this classifier is to assist in finding additional papers which have information about severity indicated through fatalities.  The underlying network works similar to popular systems which would recommend a book to a reader, however this network would be based on the internal sentiment analysis of the book they are currently reading instead of metadata tagging a book. \n",
    "\n",
    "The dataset used for this project is the COVID-19 Open Research Dataset Challenge (CORD-19) dataset as provided in the CORD-19 research challenge.  The data for the neural network has been split into training, validation and test data.  For a positive sentiment, the training data consists of papers which were found to contain the word ‘fatalities’.  The validation data papers contained the word ‘hospitilization’.  For a negative sentiment, training data contains ‘recovery’ and validation constians ‘flu’.  Test data is the entirety of papers.  An additional dataset containing the word ‘mice’ is used, since mice will never be hospitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "import sys\n",
    "from string import punctuation\n",
    "from os import listdir, mkdir, path\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Load All Papers - Cleaned and Spaced\n",
    "############################################\n",
    "\n",
    "df = pd.read_csv('spacy.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Papers used for training and validation are associated with their hash number as found in the metadata.csv (provided by the dataset)  to make it easier to locate the document in the dataset, since there are some encoding errors in the titles (such as Unicode coding issues for certain characters).  After locating the documents, they are distilled down to their basic words using a bag of words style method.  Stop words are removed using nltk’s corpus stopwords and then stored.  The words are tokenized using Kera’s Tokenizer class.  The fit_on_texts and text_to_sequences convert the text to numbers for the neural network to process.  The training data is padded so all sequences are the same length.  Classifications are stored in the ytrain and ytest variables for use in the neural network. The vocabulary used for the embedding layer contains words that occur only 3 or more times in the training set entirety of the paper. We found that using words that occured 2 or more times caused the network to overfit on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Load Paper Categories Hashes\n",
    "############################################\n",
    "\n",
    "paper_fns = ['positive_out.txt', 'negative_out.txt']\n",
    "\t\n",
    "positive_hash = None\n",
    "with open(paper_fns[0], 'r') as f:\n",
    "\tpositive_hash = f.read().split('\\n')\n",
    "\t\n",
    "negative_hash = None\n",
    "with open(paper_fns[1], 'r') as f:\n",
    "\tnegative_hash = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "316\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "### Load Papers from Hashes\n",
    "############################################\n",
    "\n",
    "hashes = df['paper_id'].values.tolist()\n",
    "paper_text = df['processed_text'].values.tolist()\n",
    "\n",
    "positive_papers = []\n",
    "\n",
    "for hash in positive_hash:\n",
    "\tfor idx,hash2 in enumerate(hashes):\n",
    "\t\tif hash == hash2:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tpositive_papers.append(paper_text[idx])\n",
    "\t\t\t\tbreak\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(hash)\n",
    "\t\t\t\t\n",
    "negative_papers = []\n",
    "for hash in negative_hash:\n",
    "\tfor idx,hash2 in enumerate(hashes):\n",
    "\t\tif hash == hash2:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tnegative_papers.append(paper_text[idx])\n",
    "\t\t\t\tbreak\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(hash)\n",
    "\t\t\t\n",
    "print(len(positive_papers))\n",
    "print(len(negative_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42745\n",
      "[('cell', 15607), ('mouse', 11669), ('infection', 11483), ('use', 11024), ('virus', 10528), ('study', 9229), ('patient', 7792), ('viral', 5971), ('group', 5213), ('protein', 5112), ('day', 5080), ('result', 4798), ('disease', 4743), ('respiratory', 4557), ('high', 4433), ('control', 4253), ('response', 4067), ('increase', 4064), ('test', 3751), ('include', 3690), ('level', 3620), ('infect', 3494), ('sample', 3467), ('report', 3363), ('expression', 3313), ('human', 3261), ('datum', 3177), ('case', 3090), ('analysis', 2993), ('antibody', 2855), ('compare', 2843), ('find', 2832), ('effect', 2808), ('time', 2748), ('model', 2715), ('lung', 2711), ('low', 2696), ('detect', 2696), ('gene', 2696), ('follow', 2668), ('clinical', 2648), ('treatment', 2595), ('activity', 2561), ('influenza', 2502), ('type', 2472), ('animal', 2447), ('observe', 2415), ('child', 2410), ('table', 2389), ('numb', 2340)]\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "### Create Vocabulary from Papers\n",
    "############################################\n",
    "\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc_vocab(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(doc, vocab):\n",
    "\ttokens = clean_doc_vocab(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs_vocab(doc_list, vocab):\n",
    "\t\tfor doc in doc_list:\n",
    "\t\t\tadd_doc_to_vocab(doc, vocab)\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs_vocab(positive_papers, vocab)\n",
    "process_docs_vocab(negative_papers, vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "\n",
    "# keep tokens with a min occurrence\n",
    "min_occurane = 1000\n",
    "vocab = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(vocab))\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Load Train, Validation, and Test sets\n",
    "############################################\n",
    "\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# filter out tokens not in vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\ttokens = ' '.join(tokens)\n",
    "\treturn tokens\n",
    "\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(doc_list, vocab):\n",
    "\tdocuments = list()\n",
    "\tfor doc in doc_list:\n",
    "\t\ttokens = clean_doc(doc, vocab)\n",
    "\t\t# add to list\n",
    "\t\tdocuments.append(tokens)\n",
    "\treturn documents\n",
    "\n",
    "# 70% Train\n",
    "# 20% Validation\n",
    "# 10% Test\n",
    "train_split = 0.7\n",
    "valid_split = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Training Set\n",
    "############################################\n",
    "\n",
    "# load all training reviews\n",
    "positive_docs = process_docs(positive_papers[:int(len(positive_papers)*train_split)], vocab)\n",
    "negative_docs = process_docs(negative_papers[:int(len(negative_papers)*train_split)], vocab)\n",
    "train_docs = negative_docs + positive_docs\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(len(positive_docs))] + [1 for _ in range(len(negative_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Validation Set\n",
    "############################################\n",
    "\n",
    "# load all test reviews\n",
    "positive_docs = process_docs(positive_papers[int(len(positive_papers)*train_split):int(len(positive_papers)*valid_split)], vocab)\n",
    "negative_docs = process_docs(negative_papers[int(len(negative_papers)*train_split):int(len(negative_papers)*valid_split)], vocab)\n",
    "valid_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(valid_docs)\n",
    "# pad sequences\n",
    "Xvalid = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "yvalid = array([0 for _ in range(len(positive_docs))] + [1 for _ in range(len(negative_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Test Set\n",
    "############################################\n",
    "\n",
    "# load all test reviews\n",
    "positive_docs = process_docs(positive_papers[int(len(positive_papers)*valid_split):], vocab)\n",
    "negative_docs = process_docs(negative_papers[int(len(negative_papers)*valid_split):], vocab)\n",
    "valid_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(valid_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(len(positive_docs))] + [1 for _ in range(len(negative_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  3 32 ...  0  0  0]\n",
      "445\n",
      "127\n",
      "[ 80  14 167 ...   0   0   0]\n",
      "64\n",
      "[ 27  12 154 ...   0   0   0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "445\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "127\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[-1])\n",
    "print(len(Xtrain))\n",
    "print(len(Xvalid))\n",
    "print(Xvalid[-1])\n",
    "print(len(Xtest))\n",
    "print(Xtest[-1])\n",
    "\n",
    "print(ytrain)\n",
    "print(len(ytrain))\n",
    "print(yvalid)\n",
    "print(len(yvalid))\n",
    "#print(ytest)\n",
    "print(len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    checkpoint_path = 'trained_models/model_1/cp.ckpt'\n",
    "    model_name = 'model_1'\n",
    "    return model, checkpoint_path, model_name\n",
    "\n",
    "\n",
    "def model2(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    checkpoint_path = 'trained_models/model_2/cp.ckpt'\n",
    "    model_name = 'model_2'\n",
    "    return model, checkpoint_path, model_name\n",
    "\n",
    "\n",
    "# doubles dense of model1 to 20\n",
    "def model3(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    checkpoint_path = 'trained_models/model_3/cp.ckpt'\n",
    "    model_name = 'model_3'\n",
    "    return model, checkpoint_path, model_name\n",
    "\n",
    "\n",
    "def model4(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=128, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Conv1D(filters=64, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    checkpoint_path = 'trained_models/model_4/cp.ckpt'\n",
    "    model_name = 'model_4'\n",
    "    return model, checkpoint_path, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Run 1D CNN\n",
    "############################################\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "epochs = 20\n",
    "\n",
    "checkpoint_path = ''\n",
    "log_dir = \"logs\"\n",
    "model_name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "The model1 neural network starts with an embedding layer, which turns positive integers in the dense vectors of a fixed size. The vocabulary size and input length are passed to the embedding.  Then 1D convolutional layer is used to do temporal convolution.  The layer creates a convolution kernel which takes the input and convulves it over a single dimension to produce a tensor of outputs.  Then, a relu activation is applied to the outputs.  A max pooling layers is used to reduce the dimensionality of the data.  Then the flatten layer collapses the spatial dimension of the input to the channel dimension. Lastly, two dense layers reduce the output to 10 and then 1 value.  The last layer uses a sigmoid to classify two categories.\n",
    "Four neural networks were tried, the results are below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model, checkpoint_path, model_name = model1(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model, checkpoint_path, model_name = model2(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model, checkpoint_path, model_name = model3(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model, checkpoint_path, model_name = model4(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 445 samples, validate on 127 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 2s - loss: 0.6799 - accuracy: 0.5663 - val_loss: 0.5862 - val_accuracy: 0.7165\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.4701 - accuracy: 0.8090 - val_loss: 0.1232 - val_accuracy: 0.9764\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.2397 - accuracy: 0.9281 - val_loss: 0.1092 - val_accuracy: 0.9764\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.1805 - accuracy: 0.9461 - val_loss: 0.1083 - val_accuracy: 0.9685\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.1297 - accuracy: 0.9596 - val_loss: 0.1289 - val_accuracy: 0.9528\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0876 - accuracy: 0.9685 - val_loss: 0.1992 - val_accuracy: 0.9449\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0680 - accuracy: 0.9775 - val_loss: 0.2274 - val_accuracy: 0.9213\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0454 - accuracy: 0.9910 - val_loss: 0.2812 - val_accuracy: 0.9370\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0429 - accuracy: 0.9865 - val_loss: 0.2991 - val_accuracy: 0.9134\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0339 - accuracy: 0.9910 - val_loss: 0.3329 - val_accuracy: 0.9291\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0176 - accuracy: 0.9933 - val_loss: 0.3717 - val_accuracy: 0.9370\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0180 - accuracy: 0.9910 - val_loss: 0.4093 - val_accuracy: 0.9370\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0348 - accuracy: 0.9843 - val_loss: 0.3844 - val_accuracy: 0.8976\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0533 - accuracy: 0.9798 - val_loss: 0.3380 - val_accuracy: 0.9291\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0312 - accuracy: 0.9865 - val_loss: 0.4492 - val_accuracy: 0.8976\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0433 - accuracy: 0.9843 - val_loss: 0.4370 - val_accuracy: 0.9055\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0247 - accuracy: 0.9865 - val_loss: 0.6320 - val_accuracy: 0.8740\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0187 - accuracy: 0.9910 - val_loss: 0.4089 - val_accuracy: 0.9291\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0192 - accuracy: 0.9910 - val_loss: 0.4974 - val_accuracy: 0.9213\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: saving model to trained_models/model_4/cp.ckpt\n",
      "445/445 - 1s - loss: 0.0133 - accuracy: 0.9933 - val_loss: 0.4633 - val_accuracy: 0.9370\n",
      "Test Accuracy: 87.500000\n"
     ]
    }
   ],
   "source": [
    "fit_dir = path.join(log_dir, \"fit\", str(model_name) + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=fit_dir, histogram_freq=1)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t save_weights_only=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t verbose=1)\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "#model.fit(Xtrain, ytrain, epochs=100, validation_data=(Xvalid, yvalid), verbose=2)\n",
    "model.fit(Xtrain, ytrain, epochs=epochs, validation_data=(Xvalid, yvalid), verbose=2,\n",
    "          callbacks=[tensorboard_callback, cp_callback])\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 15148."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network was used to classify all 16000 literatures (sans training and validation papers).  After prediction, one paper taken at random that was classified as relevant to the topic of Severity of Disease. “Impact of Middle East respiratory syndrome outbreak on the use of emergency medical resources in febrile patients”. This paper discusses the occurrence of a respiratory syndrome in 2015 in the Middle East.  From this paper we can conclude that the symptoms shown by the patients included a fever.  In addition, statistics are given regarding the duration of the fever at the emergency department and the patient’s length of stay in the emergency department.  The paper directly addresses the issue of fatality in symptomatic hospitalized patients : “We also found no change in mortality rates for febrile patients attending the ED after the outbreak.”; although there are no statistics about this topic.  The paper does highlight mortality rates for emergency room patients due to overcrowding, and cites some other papers which specifically address the issue.  The neural network classifier is specifically looking for papers regarding symptomatic patients and hospitalization, so this paper is considered useful to the Kaggle task assignment.  \n",
    "\n",
    "About half the papers were classified relevant when a threshold of .5 prediction is used.  When predicting the classes directly with keras, . This neural network would be useful to researchers looking at papers since they would have a better idea of which papers they should start to read. Looking at the top 99th percentile of papers, there are some false positives, such as “An ethnic model of Japanese overseas tourism companies”.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Loss\n",
    "\n",
    "We tested a few different neural networks and here are the validation accuracy and loss results from tensorboard (this can also be seen if line 26 is run.) Model 1 is dark blue. Model 3 is light blue. Model 4 is green. (Sorry! tensorboard colors are unpredictable!)\n",
    "\n",
    "![title](img/accuracy.png)\n",
    "![title](img/loss.png)\n",
    "\n",
    "We tested out 4 different models and included three of the models here. The best model is our first model, \"model 1\" (dark blue), which was inspired the movie sentiment analysis [1]. All three models overfit the data a fair amount because loss goes down and then rises a bit over the epochs. Below is the training and validation accuracy and loss for model 1:\n",
    "\n",
    "![title](img/model1_acc.png)\n",
    "![title](img/model1_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Labelling Process\n",
    "There did not exist labels for the papers so we had to make our own labels for training and validation. Our process was aided with the clustering kernel by maksimeren  in Kaggle [3]. First we searched in the search bar (see SHOW section in kernel) for keywords such as \"hospitalization\", \"fatality\", \"elderly\", \"mice\", etc. After searching a keyword, we looked randomly through the papers and labels papers based on whether we though the titles and skimming the papers suggested that they were about severve virus cases in humans. We had in total around 500 papers in total that we labelled (training and validation combined). \n",
    "\n",
    "While there was a determined effort not to double count our documents the time limitations of our work prohibited us both from achieving a critical mass of labeled papers and from performing significant review of documents labeled by other members. Some papers were ultimately reviewed by multiple people, mostly on accident from two people reading the same paper and then citing it in the same paper and then both rating it in the same category, we did not run into any scenarios where we had disagreements on the nature of a given paper. Moreover,\n",
    "we have no evidence that we have reached a maxima in learning accuracy and it is likely true that the accuracy of this system would only increase (and overfit less) with more labeled papers. However, given the tremendous time sink required to gather the 500 or so papers used (and how our current system, while effective under Cowens Kappa could not have been 100% effective), we would need some partially automated means of labeling more documents for the learning process. While using our current network to perform this classification process has been considered, we would prefer having more than 85% validation accuracy first. All in all it is most likely that our system will be a helpful component as a larger ensemble learning process for tagging documents once tools better than clustering for understanding the data have come into play. As opposed to a foolproof means of tagging the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "[1] Brownlee, J. (2019, November 19). How to Develop a Deep Convolutional  \n",
    "Neural Network for Sentiment Analysis (Text Classification). Retrieved May 4, 2020, from https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n",
    "\n",
    "[2] \n",
    "Li, C. H., Yang, J. C., & Park, S. C. (2011, July 22). Text categorization algorithms using semantic approaches, corpus-based thesaurus and WordNet. Retrieved from https://www.sciencedirect.com/science/article/abs/pii/S0957417411010323\n",
    "\n",
    "[3] Maksimeren. (2020, April 16). COVID-19 Literature Clustering. Retrieved from https://www.kaggle.com/maksimeren/covid-19-literature-clustering\n",
    "\n",
    "[4] Nmonath. (2020, April 17). Knowledge Discovery from Full-Text Research Papers. Retrieved from https://www.kaggle.com/nmonath/knowledge-discovery-from-full-text-research-papers\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
