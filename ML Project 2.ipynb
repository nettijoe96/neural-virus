{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying COVID19 Papers based on Severity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by Luke Batchelder, Jessica Diehl, Drew Griffith and Joe Netti\n",
    "### CSCI 635- Introduction to Machine Learning\n",
    "### Project 2- COVID-19 Open Research Dataset Challenge\n",
    "### 5/5/2020\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any technical field, researchers must keep pace with new papers. The coronavirus pandemic has generated far more papers than researchers can reasonably sift through. This project aims to help solve this problem that researchers face with the abundance of information using neural networks. Our code was based on being a bricks on bricks operation to other submissions. Based on our sources there seemed to be great tools for sorting the data into groups and generally querying on those groups and there had been solid efforts at ground up NN learning based on ensemble CNN networks. Our code was designed to test the effectiveness of combining these two functionalities by using the very effective code of (XYX) to find papers that fit our analyses and using the semantic analysis of (YXX) to find the results in order to classify papers if they discussed patients who had high severity. \n",
    "\n",
    "The task used for this project is the Risk Factors task, specifically, “Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups”. The sentiment analysis is used to provide additional papers other than ones the neural network has been trained to detect.  Does the sentiment within the high severity papers reflect that topics discussed are similar in other papers which have been found to have a similar sentiment?  Especially, do the other papers indicate information about the risk of fatality?  The high risk papers revolve around the severity of disease and contain indications of research involving fatalities or hospitalizations. The neural network used for sentiment analysis classifies papers into discussions about high severity for the disease or no discussion about the severity of the disease.  The aim for this classifier is to assist in finding additional papers which have information about severity indicated through fatalities.  The underlying network works similar to popular systems which would recommend a book to a reader, however this network would be based on the internal sentiment analysis of the book they are currently reading instead of metadata tagging a book. \n",
    "\n",
    "The dataset used for this project is the COVID-19 Open Research Dataset Challenge (CORD-19) dataset as provided in the CORD-19 research challenge.  The data for the neural network has been split into training, validation and test data.  For a positive sentiment, the training data consists of papers which were found to contain the word ‘fatalities’.  The validation data papers contained the word ‘hospitilization’.  For a negative sentiment, training data contains ‘recovery’ and validation constians ‘flu’.  Test data is the entirety of papers.  An additional dataset containing the word ‘mice’ is used, since mice will never be hospitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "import sys\n",
    "from string import punctuation\n",
    "from os import listdir, mkdir, path\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Load All Papers - Cleaned and Spaced\n",
    "############################################\n",
    "\n",
    "df = pd.read_csv('spacy.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers used for training and validation are associated with their hash number as found in the metadata.csv (provided by the dataset)  to make it easier to locate the document in the dataset, since there are some encoding errors in the titles (such as Unicode coding issues for certain characters).  After locating the documents, they are distilled down to their basic words using a bag of words style method.  Stop words are removed using nltk’s corpus stopwords and then stored.  The words are tokenized using Kera’s Tokenizer class.  The fit_on_texts and text_to_sequences convert the text to numbers for the neural network to process.  The training data is padded so all sequences are the same length.  Classifications are stored in the ytrain and ytest variables for use in the neural network. The vocabulary used for the embedding layer contains words that occur only 3 or more times in the training set entirety of the paper. We found that using words that occured 2 or more times caused the network to overfit on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Load Paper Categories Hashes\n",
    "############################################\n",
    "\n",
    "paper_fns = ['positive_out.txt', 'negative_out.txt']\n",
    "\t\n",
    "positive_hash = None\n",
    "with open(paper_fns[0], 'r') as f:\n",
    "\tpositive_hash = f.read().split('\\n')\n",
    "\t\n",
    "negative_hash = None\n",
    "with open(paper_fns[1], 'r') as f:\n",
    "\tnegative_hash = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "316\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "### Load Papers from Hashes\n",
    "############################################\n",
    "\n",
    "hashes = df['paper_id'].values.tolist()\n",
    "paper_text = df['processed_text'].values.tolist()\n",
    "\n",
    "positive_papers = []\n",
    "\n",
    "for hash in positive_hash:\n",
    "\tfor idx,hash2 in enumerate(hashes):\n",
    "\t\tif hash == hash2:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tpositive_papers.append(paper_text[idx])\n",
    "\t\t\t\tbreak\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(hash)\n",
    "\t\t\t\t\n",
    "negative_papers = []\n",
    "for hash in negative_hash:\n",
    "\tfor idx,hash2 in enumerate(hashes):\n",
    "\t\tif hash == hash2:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tnegative_papers.append(paper_text[idx])\n",
    "\t\t\t\tbreak\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(hash)\n",
    "\t\t\t\n",
    "print(len(positive_papers))\n",
    "print(len(negative_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42745\n",
      "[('cell', 15607), ('mouse', 11669), ('infection', 11483), ('use', 11024), ('virus', 10528), ('study', 9229), ('patient', 7792), ('viral', 5971), ('group', 5213), ('protein', 5112), ('day', 5080), ('result', 4798), ('disease', 4743), ('respiratory', 4557), ('high', 4433), ('control', 4253), ('response', 4067), ('increase', 4064), ('test', 3751), ('include', 3690), ('level', 3620), ('infect', 3494), ('sample', 3467), ('report', 3363), ('expression', 3313), ('human', 3261), ('datum', 3177), ('case', 3090), ('analysis', 2993), ('antibody', 2855), ('compare', 2843), ('find', 2832), ('effect', 2808), ('time', 2748), ('model', 2715), ('lung', 2711), ('low', 2696), ('detect', 2696), ('gene', 2696), ('follow', 2668), ('clinical', 2648), ('treatment', 2595), ('activity', 2561), ('influenza', 2502), ('type', 2472), ('animal', 2447), ('observe', 2415), ('child', 2410), ('table', 2389), ('numb', 2340)]\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "### Create Vocabulary from Papers\n",
    "############################################\n",
    "\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc_vocab(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(doc, vocab):\n",
    "\ttokens = clean_doc_vocab(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs_vocab(doc_list, vocab):\n",
    "\t\tfor doc in doc_list:\n",
    "\t\t\tadd_doc_to_vocab(doc, vocab)\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs_vocab(positive_papers, vocab)\n",
    "process_docs_vocab(negative_papers, vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "\n",
    "# keep tokens with a min occurrence\n",
    "min_occurane = 1000\n",
    "vocab = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(vocab))\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Load Train, Validation, and Test sets\n",
    "############################################\n",
    "\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# filter out tokens not in vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\ttokens = ' '.join(tokens)\n",
    "\treturn tokens\n",
    "\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(doc_list, vocab):\n",
    "\tdocuments = list()\n",
    "\tfor doc in doc_list:\n",
    "\t\ttokens = clean_doc(doc, vocab)\n",
    "\t\t# add to list\n",
    "\t\tdocuments.append(tokens)\n",
    "\treturn documents\n",
    "\n",
    "# 70% Train\n",
    "# 20% Validation\n",
    "# 10% Test\n",
    "train_split = 0.7\n",
    "valid_split = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Training Set\n",
    "############################################\n",
    "\n",
    "# load all training reviews\n",
    "positive_docs = process_docs(positive_papers[:int(len(positive_papers)*train_split)], vocab)\n",
    "negative_docs = process_docs(negative_papers[:int(len(negative_papers)*train_split)], vocab)\n",
    "train_docs = negative_docs + positive_docs\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(len(positive_docs))] + [1 for _ in range(len(negative_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Validation Set\n",
    "############################################\n",
    "\n",
    "# load all test reviews\n",
    "positive_docs = process_docs(positive_papers[int(len(positive_papers)*train_split):int(len(positive_papers)*valid_split)], vocab)\n",
    "negative_docs = process_docs(negative_papers[int(len(negative_papers)*train_split):int(len(negative_papers)*valid_split)], vocab)\n",
    "valid_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(valid_docs)\n",
    "# pad sequences\n",
    "Xvalid = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "yvalid = array([0 for _ in range(len(positive_docs))] + [1 for _ in range(len(negative_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Test Set\n",
    "############################################\n",
    "\n",
    "# load all test reviews\n",
    "positive_docs = process_docs(positive_papers[int(len(positive_papers)*valid_split):], vocab)\n",
    "negative_docs = process_docs(negative_papers[int(len(negative_papers)*valid_split):], vocab)\n",
    "valid_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(valid_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(len(positive_docs))] + [1 for _ in range(len(negative_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  3 32 ...  0  0  0]\n",
      "445\n",
      "127\n",
      "[ 80  14 167 ...   0   0   0]\n",
      "64\n",
      "[ 27  12 154 ...   0   0   0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "445\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "127\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[-1])\n",
    "print(len(Xtrain))\n",
    "print(len(Xvalid))\n",
    "print(Xvalid[-1])\n",
    "print(len(Xtest))\n",
    "print(Xtest[-1])\n",
    "\n",
    "print(ytrain)\n",
    "print(len(ytrain))\n",
    "print(yvalid)\n",
    "print(len(yvalid))\n",
    "#print(ytest)\n",
    "print(len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def model2(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# doubles dense of model1 to 20\n",
    "def model3(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def model4(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=128, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Conv1D(filters=64, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Run 1D CNN\n",
    "############################################\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "epochs = 20\n",
    "\n",
    "log_dir = \"logs\"\n",
    "model_name = \"model_1\"\n",
    "fit_dir = path.join(log_dir, \"fit\", str(model_name) + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=fit_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network starts with an embedding layer, which turns positive integers in the dense vectors of a fixed size. The vocabulary size and input length are passed to the embedding.  Then 1D convolutional layer is used to do temporal convolution.  The layer creates a convolution kernel which takes the input and convulves it over a single dimension to produce a tensor of outputs.  Then, a relu activation is applied to the outputs.  A max pooling layers is used to reduce the dimensionality of the data.  Then the flatten layer collapses the spatial dimension of the input to the channel dimension. Lastly, two dense layers reduce the output to 10 and then 1 value.  The last layer uses a sigmoid to classify two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = model1(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = model2(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = model3(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = model4(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 445 samples, validate on 127 samples\n",
      "Epoch 1/20\n",
      "445/445 - 1s - loss: 0.6336 - accuracy: 0.6292 - val_loss: 0.3845 - val_accuracy: 0.8898\n",
      "Epoch 2/20\n",
      "445/445 - 0s - loss: 0.3350 - accuracy: 0.9034 - val_loss: 0.1085 - val_accuracy: 0.9764\n",
      "Epoch 3/20\n",
      "445/445 - 0s - loss: 0.1731 - accuracy: 0.9438 - val_loss: 0.1634 - val_accuracy: 0.9370\n",
      "Epoch 4/20\n",
      "445/445 - 0s - loss: 0.1280 - accuracy: 0.9573 - val_loss: 0.1177 - val_accuracy: 0.9528\n",
      "Epoch 5/20\n",
      "445/445 - 0s - loss: 0.0815 - accuracy: 0.9730 - val_loss: 0.1771 - val_accuracy: 0.9370\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "#model.fit(Xtrain, ytrain, epochs=100, validation_data=(Xvalid, yvalid), verbose=2)\n",
    "model.fit(Xtrain, ytrain, epochs=epochs, validation_data=(Xvalid, yvalid), verbose=2, callbacks=[tensorboard_callback])\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1976), started 0:01:21 ago. (Use '!kill 1976' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d3dc58fd0236ec9a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d3dc58fd0236ec9a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network was used to classify all 16000 literatures (sans training and validation papers).  After prediction, one paper taken at random that was classified as relevant to the topic of Severity of Disease. “Impact of Middle East respiratory syndrome outbreak on the use of emergency medical resources in febrile patients”. This paper discusses the occurrence of a respiratory syndrome in 2015 in the Middle East.  From this paper we can conclude that the symptoms shown by the patients included a fever.  In addition, statistics are given regarding the duration of the fever at the emergency department and the patient’s length of stay in the emergency department.  The paper directly addresses the issue of fatality in symptomatic hospitalized patients : “We also found no change in mortality rates for febrile patients attending the ED after the outbreak.”; although there are no statistics about this topic.  The paper does highlight mortality rates for emergency room patients due to overcrowding, and cites some other papers which specifically address the issue.  The neural network classifier is specifically looking for papers regarding symptomatic patients and hospitalization, so this paper is considered useful to the Kaggle task assignment.  \n",
    "\n",
    "About half the papers were classified relevant when a threshold of .5 prediction is used.  When predicting the classes directly with keras, . This neural network would be useful to researchers looking at papers since they would have a better idea of which papers they should start to read. Looking at the top 99th percentile of papers, there are some false positives, such as “An ethnic model of Japanese overseas tourism companies”.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Loss\n",
    "\n",
    "We tested a few different neural networks and here are the validation accuracy and loss results from tensorboard (this can also be seen if line 26 is run.) \n",
    "![title](accuracy.png)\n",
    "![title](loss.png)\n",
    "We tested out 4 different models and included three of the models here. The best model is our first model, \"model 1\", which was inspired the movie sentiment analysis [1].  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorizing training data\n",
    "There did not exist labels for the papers so we had to make our own labels for training and validation. Our process was aided with the clustering kernel by maksimeren  in Kaggle [2]. First we searched in the search bar (see SHOW section in kernel) for keywords such as \"hospitalization\", \"fatality\", \"elderly\", \"mice\", etc. After searching a keyword, we looked randomly through the papers and labels papers based on whether we though the titles and skimming the papers suggested that they were about severve virus cases in humans.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "[1] Brownlee, J. (2019, November 19). How to Develop a Deep Convolutional  \n",
    "Neural Network for Sentiment Analysis (Text Classification). Retrieved May 4, 2020, from https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n",
    "[2] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
