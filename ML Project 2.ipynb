{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "import sys\n",
    "from string import punctuation\n",
    "from os import listdir, mkdir, path\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Load All Papers - Cleaned and Spaced\n",
    "############################################\n",
    "\n",
    "df = pd.read_csv('spacy.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Load Paper Categories Hashes\n",
    "############################################\n",
    "\n",
    "paper_fns = ['positive_out.txt', 'negative_out.txt']\n",
    "\t\n",
    "positive_hash = None\n",
    "with open(paper_fns[0], 'r') as f:\n",
    "\tpositive_hash = f.read().split('\\n')\n",
    "\t\n",
    "negative_hash = None\n",
    "with open(paper_fns[1], 'r') as f:\n",
    "\tnegative_hash = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "316\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "### Load Papers from Hashes\n",
    "############################################\n",
    "\n",
    "hashes = df['paper_id'].values.tolist()\n",
    "paper_text = df['processed_text'].values.tolist()\n",
    "\n",
    "positive_papers = []\n",
    "\n",
    "for hash in positive_hash:\n",
    "\tfor idx,hash2 in enumerate(hashes):\n",
    "\t\tif hash == hash2:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tpositive_papers.append(paper_text[idx])\n",
    "\t\t\t\tbreak\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(hash)\n",
    "\t\t\t\t\n",
    "negative_papers = []\n",
    "for hash in negative_hash:\n",
    "\tfor idx,hash2 in enumerate(hashes):\n",
    "\t\tif hash == hash2:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tnegative_papers.append(paper_text[idx])\n",
    "\t\t\t\tbreak\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(hash)\n",
    "\t\t\t\n",
    "print(len(positive_papers))\n",
    "print(len(negative_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42745\n",
      "[('cell', 15607), ('mouse', 11669), ('infection', 11483), ('use', 11024), ('virus', 10528), ('study', 9229), ('patient', 7792), ('viral', 5971), ('group', 5213), ('protein', 5112), ('day', 5080), ('result', 4798), ('disease', 4743), ('respiratory', 4557), ('high', 4433), ('control', 4253), ('response', 4067), ('increase', 4064), ('test', 3751), ('include', 3690), ('level', 3620), ('infect', 3494), ('sample', 3467), ('report', 3363), ('expression', 3313), ('human', 3261), ('datum', 3177), ('case', 3090), ('analysis', 2993), ('antibody', 2855), ('compare', 2843), ('find', 2832), ('effect', 2808), ('time', 2748), ('model', 2715), ('lung', 2711), ('low', 2696), ('detect', 2696), ('gene', 2696), ('follow', 2668), ('clinical', 2648), ('treatment', 2595), ('activity', 2561), ('influenza', 2502), ('type', 2472), ('animal', 2447), ('observe', 2415), ('child', 2410), ('table', 2389), ('numb', 2340)]\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "### Create Vocabulary from Papers\n",
    "############################################\n",
    "\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc_vocab(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(doc, vocab):\n",
    "\ttokens = clean_doc_vocab(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs_vocab(doc_list, vocab):\n",
    "\t\tfor doc in doc_list:\n",
    "\t\t\tadd_doc_to_vocab(doc, vocab)\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs_vocab(positive_papers, vocab)\n",
    "process_docs_vocab(negative_papers, vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "\n",
    "# keep tokens with a min occurrence\n",
    "min_occurane = 1000\n",
    "vocab = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(vocab))\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Load Train, Validation, and Test sets\n",
    "############################################\n",
    "\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# filter out tokens not in vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\ttokens = ' '.join(tokens)\n",
    "\treturn tokens\n",
    "\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(doc_list, vocab):\n",
    "\tdocuments = list()\n",
    "\tfor doc in doc_list:\n",
    "\t\ttokens = clean_doc(doc, vocab)\n",
    "\t\t# add to list\n",
    "\t\tdocuments.append(tokens)\n",
    "\treturn documents\n",
    "\n",
    "# 70% Train\n",
    "# 20% Validation\n",
    "# 10% Test\n",
    "train_split = 0.7\n",
    "valid_split = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Training Set\n",
    "############################################\n",
    "\n",
    "# load all training reviews\n",
    "positive_docs = process_docs(positive_papers[:int(len(positive_papers)*train_split)], vocab)\n",
    "negative_docs = process_docs(negative_papers[:int(len(negative_papers)*train_split)], vocab)\n",
    "train_docs = negative_docs + positive_docs\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(len(positive_docs))] + [1 for _ in range(len(negative_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Validation Set\n",
    "############################################\n",
    "\n",
    "# load all test reviews\n",
    "positive_docs = process_docs(positive_papers[int(len(positive_papers)*train_split):int(len(positive_papers)*valid_split)], vocab)\n",
    "negative_docs = process_docs(negative_papers[int(len(negative_papers)*train_split):int(len(negative_papers)*valid_split)], vocab)\n",
    "valid_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(valid_docs)\n",
    "# pad sequences\n",
    "Xvalid = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "yvalid = array([0 for _ in range(len(positive_docs))] + [1 for _ in range(len(negative_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Test Set\n",
    "############################################\n",
    "\n",
    "# load all test reviews\n",
    "positive_docs = process_docs(positive_papers[int(len(positive_papers)*valid_split):], vocab)\n",
    "negative_docs = process_docs(negative_papers[int(len(negative_papers)*valid_split):], vocab)\n",
    "valid_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(valid_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(len(positive_docs))] + [1 for _ in range(len(negative_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  3 32 ...  0  0  0]\n",
      "445\n",
      "127\n",
      "[ 80  14 167 ...   0   0   0]\n",
      "64\n",
      "[ 27  12 154 ...   0   0   0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "445\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "127\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[-1])\n",
    "print(len(Xtrain))\n",
    "print(len(Xvalid))\n",
    "print(Xvalid[-1])\n",
    "print(len(Xtest))\n",
    "print(Xtest[-1])\n",
    "\n",
    "print(ytrain)\n",
    "print(len(ytrain))\n",
    "print(yvalid)\n",
    "print(len(yvalid))\n",
    "#print(ytest)\n",
    "print(len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def model2(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# doubles dense of model1 to 20\n",
    "def model3(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def model4(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=128, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Conv1D(filters=64, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### Run 1D CNN\n",
    "############################################\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "epochs = 20\n",
    "\n",
    "log_dir = \"logs\"\n",
    "model_name = \"model_1\"\n",
    "fit_dir = path.join(log_dir, \"fit\", str(model_name) + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=fit_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = model1(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = model2(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = model3(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = model4(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 445 samples, validate on 127 samples\n",
      "Epoch 1/20\n",
      "445/445 - 1s - loss: 0.6336 - accuracy: 0.6292 - val_loss: 0.3845 - val_accuracy: 0.8898\n",
      "Epoch 2/20\n",
      "445/445 - 0s - loss: 0.3350 - accuracy: 0.9034 - val_loss: 0.1085 - val_accuracy: 0.9764\n",
      "Epoch 3/20\n",
      "445/445 - 0s - loss: 0.1731 - accuracy: 0.9438 - val_loss: 0.1634 - val_accuracy: 0.9370\n",
      "Epoch 4/20\n",
      "445/445 - 0s - loss: 0.1280 - accuracy: 0.9573 - val_loss: 0.1177 - val_accuracy: 0.9528\n",
      "Epoch 5/20\n",
      "445/445 - 0s - loss: 0.0815 - accuracy: 0.9730 - val_loss: 0.1771 - val_accuracy: 0.9370\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "#model.fit(Xtrain, ytrain, epochs=100, validation_data=(Xvalid, yvalid), verbose=2)\n",
    "model.fit(Xtrain, ytrain, epochs=epochs, validation_data=(Xvalid, yvalid), verbose=2, callbacks=[tensorboard_callback])\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1976), started 0:01:21 ago. (Use '!kill 1976' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d3dc58fd0236ec9a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d3dc58fd0236ec9a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
